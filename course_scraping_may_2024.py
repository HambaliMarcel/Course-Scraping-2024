# -*- coding: utf-8 -*-
"""Course Scraping May 2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12mYd6anGtyam9IbLsWPK7tmHYCqeRce-
"""

# MARITIME TRAINING ACADEMY

# Step 1: Install necessary libraries
!pip install selenium beautifulsoup4 pandas openpyxl

# Step 2: Import libraries
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from google.colab import files

# Step 3: Set up Selenium WebDriver
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Download the ChromeDriver from https://sites.google.com/a/chromium.org/chromedriver/ and set the executable path
driver = webdriver.Chrome(service=Service(), options=chrome_options)

# List of courses
courses = [
    "art-on-superyachts", "boatyard-and-marina-operations", "health-and-safety-in-ship-operations",
    "lng-shipping", "marine-insurance-claims", "marine-pilotage", "marine-salvage",
    "maritime-fire-prevention-fire-fighting-and-fire-safety", "maritime-law", "offshore-operations",
    "offshore-wind-energy", "port-state-control", "restoration-of-historic-ships-boats",
    "ship-security", "ship-surveying", "shipbuilding-and-ship-repair", "superyacht-management",
    "superyacht-project-management-refit-and-newbuilding", "superyacht-pursers", "superyacht-surveying",
    "superyacht-operations", "tanker-operations", "technical-ship-management", "yacht-boat-building",
    "yacht-small-craft-surveying", "yacht-brokerage", "cargo-surveying", "conducting-an-inclining-test",
    "introduction-to-port-state-inspection", "introduction-to-the-ism-code", "introduction-to-the-superyacht-industry",
    "introduction-to-ship-surveying", "introduction-to-the-inventory-of-hazardous-materials-on-ships",
    "marine-incident-investigation", "maritime-emergency-preparation-and-response", "sails-and-rigs",
    "superyacht-deckhands", "surveying-yacht-and-small-craft-engines", "surveying-yacht-and-small-craft-systems"
]

base_url = "https://maritimetrainingacademy.com/courses/"

course_names = []
descriptions = []
outlines = []

for course in courses:
    url = base_url + course
    driver.get(url)
    time.sleep(3)  # Wait for the page to load

    # Parse the page source with BeautifulSoup
    soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Extract Course Name
    course_name_tag = soup.select_one('.et_pb_text_0 h1')
    course_name = course_name_tag.text.strip() if course_name_tag else 'No course name available'
    course_names.append(course_name)

    # Extract all Description elements
    description_tags = soup.select('.et_pb_text_inner h2 + p')
    description = ' '.join([tag.get_text(strip=True) for tag in description_tags]) if description_tags else 'No description available'
    descriptions.append(description)

    # Extract all Outline elements
    outline_tags = soup.select('h5.et_pb_toggle_title')
    if outline_tags:
        outline = ', '.join([tag.get_text(strip=True) for tag in outline_tags])
    else:
        # If no outline tags, extract <li> elements
        li_tags = soup.select('li')
        outline = ', '.join([tag.get_text(strip=True) for tag in li_tags]) if li_tags else 'No outline available'

    outlines.append(outline)

driver.quit()

# Step 4: Save data to an Excel file
df = pd.DataFrame({
    'Course Name': course_names,
    'Description': descriptions,
    'Outline': outlines
})

df.to_excel('maritime_courses.xlsx', index=False)

# Provide download link
files.download('maritime_courses.xlsx')

# APEC PORT TRAINING

# Step 1: Install necessary libraries
!pip install selenium beautifulsoup4 pandas openpyxl

# Step 2: Import libraries
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from google.colab import files

# Step 3: Set up Selenium WebDriver
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Download the ChromeDriver from https://sites.google.com/a/chromium.org/chromedriver/ and set the executable path
driver = webdriver.Chrome(service=Service(), options=chrome_options)

# Website URL
url = "https://apecporttraining.com/courses/"

# Visit the website
driver.get(url)
time.sleep(3)  # Wait for the page to load

# Parse the page source with BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Extracting course links
course_links = [link['href'] for link in soup.select('.u-url.card-link.stretched-link.text-reset.m-0.c-card__link')]

# Initialize lists to store data
course_names = []
descriptions = []
outlines = []

# Iterate through each course link
for course_link in course_links:
    # Visit the course page
    driver.get(course_link)
    time.sleep(3)  # Wait for the page to load

    # Parse the course page source with BeautifulSoup
    course_soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Extract Course Name
    course_name_tag = course_soup.find(class_='hentry-title')
    course_name = course_name_tag.text.strip() if course_name_tag else 'No course name available'
    course_names.append(course_name)

    # Extract Description
    description_tags = course_soup.select('.container p')
    description = ' '.join([tag.get_text(strip=True) for tag in description_tags]) if description_tags else 'No description available'
    descriptions.append(description)

    # Extract Outline
    outline_tags = course_soup.select('.container strong')
    outline = ' '.join([tag.get_text(strip=True) for tag in outline_tags]) if outline_tags else 'No outline available'
    outlines.append(outline)

# Close the webdriver
driver.quit()

# Step 4: Save data to an Excel file
df = pd.DataFrame({
    'Course Name': course_names,
    'Description': descriptions,
    'Outline': outlines
})

df.to_excel('apecport_courses.xlsx', index=False)

# Provide download link
files.download('apecport_courses.xlsx')

# STC NEXT INTERNATIONAL

# Step 1: Install necessary libraries
!pip install selenium beautifulsoup4 pandas openpyxl

# Step 2: Import libraries
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from google.colab import files

# Step 3: Set up Selenium WebDriver
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Download the ChromeDriver from https://sites.google.com/a/chromium.org/chromedriver/ and set the executable path
driver = webdriver.Chrome(service=Service(), options=chrome_options)

# Website URL
url = "https://stc-international.nl/courses/"

# Visit the website
driver.get(url)
time.sleep(3)  # Wait for the page to load

# Parse the page source with BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Extracting course links
course_links = [link['href'] for link in soup.select('.w-grid-item-anchor')]

# Initialize lists to store data
course_names = []
descriptions = []
outlines = []

# Iterate through each course link
for course_link in course_links:
    # Visit the course page
    driver.get(course_link)
    time.sleep(3)
    course_soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Extract Course Name
    course_name_tag = course_soup.find(class_='w-post-elm post_title entry-title color_link_inherit')
    course_name = course_name_tag.text.strip() if course_name_tag else 'No course name available'
    course_names.append(course_name)

    # Extract Description
    description_tags = course_soup.select('.mc--content p')
    description = ' '.join([tag.get_text(strip=True) for tag in description_tags]) if description_tags else 'No description available'
    descriptions.append(description)

    # Extract Outline
    outline_tags = course_soup.select('.mc--content li')
    outline = ' '.join([tag.get_text(strip=True) for tag in outline_tags]) if outline_tags else 'No outline available'
    outlines.append(outline)

# Close the webdriver
driver.quit()

df = pd.DataFrame({
    'Course Name': course_names,
    'Description': descriptions,
    'Outline': outlines
})

df.to_excel('stc-international.xlsx', index=False)

files.download('stc-international.xlsx')

# FORCE TECHNOLOGY

# Step 1: Install necessary libraries
!pip install selenium beautifulsoup4 pandas openpyxl

# Step 2: Import libraries
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By  # Import the By class
from bs4 import BeautifulSoup
from google.colab import files

# Step 3: Set up Selenium WebDriver
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Download the ChromeDriver from https://sites.google.com/a/chromium.org/chromedriver/ and set the executable path
driver = webdriver.Chrome(service=Service(), options=chrome_options)

# Website URL
url = "https://forcetechnology.com/en/services/simulations-and-cfd/"
url2 = "https://forcetechnology.com/"

# Visit the website
driver.get(url)
time.sleep(3)
print("URL :"+url)

accept_button = driver.find_element(By.CLASS_NAME, 'coi-banner__accept')
if accept_button:
    accept_button.click()
    time.sleep(2)  # Wait for the banner to close

# Parse the page source with BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Extracting course links
course_links = [link['href'] for link in soup.select('.incognito')]

# Initialize lists to store data
course_names = []
descriptions = []
outlines = []

# Iterate through each course link
for course_link in course_links:
    # Visit the course page
    url3 = url2 + course_link
    print("Course Link:"+url3)
    driver.get(url3)
    time.sleep(3)
    course_soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Extract Course Name
    course_name_tag = course_soup.find(class_='page-title')
    course_name = course_name_tag.text.strip() if course_name_tag else 'No course name available'
    course_names.append(course_name)

    # Extract Description
    description_tags = course_soup.find(class_='bodytext')
    description = ' '.join([tag.get_text(strip=True) for tag in description_tags]) if description_tags else 'No description available'
    descriptions.append(description)

    # Extract Outline
    outline_tags = course_soup.find(class_='bodytext')
    outline = ' '.join([tag.get_text(strip=True) for tag in outline_tags]) if outline_tags else 'No outline available'
    outlines.append(outline)

# Close the webdriver
driver.quit()

df = pd.DataFrame({
    'Course Name': course_names,
    'Description': descriptions,
    'Outline': outlines
})

df.to_excel('force-technology.xlsx', index=False)

files.download('force-technology.xlsx')

# https://alison.com/courses?tag=supply-chain-management&tag=aviation&tag=shipping&tag=maritime-law&tag=sdg-14-life-below-water&tag=marine-engineering&language=en&query=port


# Step 1: Install necessary libraries
!pip install selenium beautifulsoup4 pandas openpyxl

# Step 2: Import libraries
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By  # Import the By class
from bs4 import BeautifulSoup
from google.colab import files

# Step 3: Set up Selenium WebDriver
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")

# Download the ChromeDriver from https://sites.google.com/a/chromium.org/chromedriver/ and set the executable path
driver = webdriver.Chrome(service=Service(), options=chrome_options)

# Website URL
url = "https://alison.com/courses?tag=supply-chain-management&tag=aviation&tag=shipping&tag=maritime-law&tag=sdg-14-life-below-water&tag=marine-engineering&language=en&query=port"
url2 = "https://alison.com/courses"

# Visit the website
driver.get(url)
time.sleep(3)
print("URL :"+url)

# Parse the page source with BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')

# Extracting course links
course_links = [link['href'] for link in soup.select('.card__more--mobile')]

# Initialize lists to store data
course_names = []
descriptions = []
outlines = []

# Iterate through each course link
for course_link in course_links:
    # Visit the course page
    url3 = url2 + course_link
    print("Course Link:"+ course_link)
    driver.get(url3)
    time.sleep(4)
    course_soup = BeautifulSoup(driver.page_source, 'html.parser')

    # Extract Course Name
    course_name_tag = course_soup.find(class_='course-title')
    course_name = course_name_tag.text.strip() if course_name_tag else 'No course name available'
    course_names.append(course_name)


    # Extract Outline
    outline_tags = course_soup.find(class_='l-mods__topics-inner')
    outline = ' '.join([tag.get_text(strip=True) for tag in outline_tags]) if outline_tags else 'No outline available'
    outlines.append(outline)

    accept_button = driver.find_element(By.CSS_SELECTOR, '[data-tab="course-desc"]')
    if accept_button:
        accept_button.click()
        time.sleep(2)

    # Extract Description
    description_tags = course_soup.find(class_='course-desc')
    description = ' '.join([tag.get_text(strip=True) for tag in description_tags]) if description_tags else 'No description available'
    descriptions.append(description)

# Close the webdriver
driver.quit()

df = pd.DataFrame({
    'Course Name': course_names,
    'Description': descriptions,
    'Outline': outlines
})

df.to_excel('alison.xlsx', index=False)

files.download('alison.xlsx')